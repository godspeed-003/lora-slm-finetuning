# Dataset Configuration for LoRA Fine-tuning
# OPTIMIZED FOR COLAB FREE (3-4 hour training time)

datasets:
  # Core LM Training (3 datasets)
  - name: "wikitext"
    config: "wikitext-103-raw-v1"
    split: "train"
    text_field: "text"

  - name: "roneneldan/TinyStories"
    config: "default"
    split: "train"
    text_field: "text"

  # News (2 datasets)
  - name: "ag_news"
    config: "default"
    split: "train"
    text_field: "text"

  - name: "yelp_review_full"
    config: "default"
    split: "train"
    text_field: "text"

  # Summarization (2 datasets)
  - name: "cnn_dailymail"
    config: "3.0.0"
    split: "train"
    text_field: "article"

  - name: "xsum"
    config: "default"
    split: "train"
    text_field: "document"

  # QA & Sentiment (2 datasets)
  - name: "squad"
    config: "plain_text"
    split: "train"
    text_field: "context"

  - name: "imdb"
    config: "plain_text"
    split: "train"
    text_field: "text"

# CRITICAL: Reduce sample size for Colab constraints
max_samples_per_dataset: 10000 # ‚Üê CHANGED FROM 50000
validation_split: 0.15
