\documentclass{article}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{geometry}

\geometry{a4paper, margin=1in}

\title{LoRA Fine-Tuning of GPT-2: A Production Implementation}
\author{Vedant Korade}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This report presents a production-grade implementation of Low-Rank Adaptation (LoRA) for fine-tuning a small language model (GPT-2, 124M parameters) on a diverse mixture of eight text datasets. By freezing 99.35\% of the model parameters and training only 0.65\% (811k parameters), we achieve competitive performance with a final validation perplexity of \textbf{21.20} and a validation token loss of \textbf{3.05}. The system demonstrates efficient adaptation to new domains while maintaining high throughput (approx. 43.5 samples/second) during inference.
\end{abstract}

\section{Introduction}
Fine-tuning Large Language Models (LLMs) is computationally expensive and storage-intensive. Parameter-Efficient Fine-Tuning (PEFT) methods, particularly Low-Rank Adaptation (LoRA), offer a solution by injecting trainable rank decomposition matrices into each layer of the Transformer architecture while keeping the pre-trained weights frozen. This project implements LoRA to adapt GPT-2 to a broad range of text generation tasks without the overhead of full fine-tuning.

\section{Methodology}

\subsection{Model Architecture}
We utilized the \texttt{gpt2} base model (124 million parameters). LoRA was applied to the attention projection layers (\texttt{c\_attn}) and output projection layers (\texttt{c\_proj}) with the following configuration:
\begin{itemize}
    \item \textbf{Rank ($r$)}: 8
    \item \textbf{Alpha ($\alpha$)}: 16
    \item \textbf{Dropout}: 0.1
    \item \textbf{Bias}: None
    \item \textbf{Task Type}: CAUSAL\_LM
\end{itemize}
This resulted in approximately 811,008 trainable parameters, representing 0.65\% of the total model size.

\subsection{Datasets}
We curated a diverse mixture of eight non-reasoning text datasets from the Hugging Face Hub. To ensure balanced training within a constrained compute budget (Google Colab T4 GPU), each dataset was sampled to a maximum of 10,000 examples.

\begin{table}[h]
\centering
\begin{tabular}{ll}
\toprule
\textbf{Dataset} & \textbf{Domain/Task} \\
\midrule
Salesforce/wikitext (wikitext-103) & Encyclopedia \\
roneneldan/TinyStories & Fiction (Simple Narratives) \\
ag\_news & News Classification \\
yelp\_review\_full & Sentiment/Reviews \\
cnn\_dailymail & Summarization \\
xsum & Extreme Summarization \\
squad & Question Answering Contexts \\
imdb & Movie Reviews \\
\bottomrule
\end{tabular}
\caption{Dataset Mixture}
\end{table}

\subsection{Training Setup}
Training was conducted using the Hugging Face \texttt{Trainer} on a single GPU (T4).

\textbf{Hyperparameters:}
\begin{itemize}
    \item \textbf{Epochs}: 3
    \item \textbf{Learning Rate}: 3.0e-4 (AdamW optimizer)
    \item \textbf{Batch Size}: 4 per device
    \item \textbf{Gradient Accumulation}: 8 steps (Effective Batch Size: 32)
    \item \textbf{Precision}: FP16 (Mixed Precision)
    \item \textbf{Sequence Length}: 512 tokens
    \item \textbf{Weight Decay}: 0.01
    \item \textbf{Warmup Steps}: 100
\end{itemize}

\section{Results}

\subsection{Training Metrics}
The model converged stably over 6,370 steps. The final validation metrics on the held-out test set (15\% split) are:

\begin{itemize}
    \item \textbf{Validation Token Loss}: 3.0542
    \item \textbf{Validation Perplexity (PPL)}: 21.2037
    \item \textbf{Bits Per Token (BPT)}: 4.4062
\end{itemize}

\begin{figure}[h]
    \centering
    % Suggested W&B Plot: train/loss vs step
    \includegraphics[width=0.8\textwidth]{train_loss.png} 
    \caption{Training Loss over global steps, showing steady convergence.}
    \label{fig:train_loss}
\end{figure}

\begin{figure}[h]
    \centering
    % Suggested W&B Plot: eval/loss vs step
    \includegraphics[width=0.8\textwidth]{eval_loss.png}
    \caption{Validation Loss over global steps.}
    \label{fig:eval_loss}
\end{figure}

\subsection{Inference Performance}
Inference efficiency was measured on the evaluation set. The model achieved an average throughput of \textbf{43.49 samples/second} (10.87 steps/second), confirming that the addition of LoRA adapters introduced negligible latency overhead compared to the base model.

\section{Discussion}
The results demonstrate that low-rank adaptation is highly effective for generalize fine-tuning across multiple domains. A perplexity of 21.20 is competitive for a model of this size (124M parameters) given the diversity of the input data (news, fiction, reviews, encyclopedia). The training process was stable, and the model did not exhibit signs of catastrophic forgetting or instability, validating the choice of hyperparameters ($r=8, \alpha=16$).

\section{Conclusion}
We have successfully implemented a production-ready pipeline for fine-tuning SLMs using LoRA. The approach proves to be both parameter-efficient and computationally viable for consumer-grade hardware, making it an excellent strategy for domain-specific adaptation of foundational models.

\end{document}
